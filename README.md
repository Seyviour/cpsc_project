# Claim Verification and Evidence Analysis Pipeline

## Project Overview

This project implements a pipeline for analyzing and verifying claims based on provided evidence. The core idea is to process textual claims and associated knowledge documents to:
1.  Filter and identify the most relevant segments of evidence for a given claim.
2.  Generate Question-Answer (QANDA) pairs from the claim and its supporting evidence.
3.  Optionally rephrase claims for robustness or to augment data.
4.  Ultimately classify the claim (e.g., as Supported, Refuted) based on the generated QANDA pairs, leveraging a Large Language Model (LLM).

The pipeline involves several Python scripts, each performing a distinct step in the process, from initial data filtering to final claim classification.

## Scripts Description

The project consists of the following Python scripts:

*   **`filter_docs.py`**:
    *   **Role**: Processes input claims and a knowledge store of documents to identify the most relevant text spans (evidence snippets) for each claim.
    *   **Details**: It uses Natural Language Processing (NLP) techniques, including FastText word embeddings for semantic representation, NLTK for text preprocessing (tokenization, stopword removal, stemming), and the Hungarian algorithm (`scipy.optimize.linear_sum_assignment`) for optimal matching between words in claims and document patches. It also supports using precomputed word similarity caches and processing data in parallelizable chunks.
    *   **Input**: Claim data (JSON format), knowledge store documents (organized in ZIP archives), and optionally, precomputed word similarity files and claim rephrasals.
    *   **Output**: Pandas DataFrame pickle files (`.pkl`) containing the claims, their original labels, and the filtered/extracted top matching document snippets.

*   **`run_qanda.py`**:
    *   **Role**: Generates Question and Answer (QANDA) pairs based on a claim and its associated relevant document extracts (evidence).
    *   **Details**: It takes claims and their corresponding evidence snippets (likely from `filter_docs.py`), formats them as prompts for a pre-trained Large Language Model (LLM), and instructs the LLM to generate a relevant QANDA pair.
    *   **Input**: A pickle file (e.g., `val.withextracts.pickle`) containing claims and their relevant document extracts.
    *   **Output**: A CSV file (`qanda_results.csv`) storing each original claim, the specific document extract used, and the LLM-generated QANDA pair.

*   **`run_rephrasals.py`**:
    *   **Role**: Generates multiple diverse rephrased versions of input claims.
    *   **Details**: It uses a pre-trained LLM to rewrite claims in different ways while preserving their original meaning. This can be useful for data augmentation or testing model robustness.
    *   **Input**: A pickle file (e.g., `val.withextracts.pickle`) containing claims.
    *   **Output**: A CSV file (`rephrasals_allresults.csv`) containing the original claim data along with a string of LLM-generated rephrasals.

*   **`run_finalclassification.py`**:
    *   **Role**: Classifies claims into predefined categories (e.g., Supported, Refuted, Not Enough Evidence, Conflicting Evidence/Cherrypicking).
    *   **Details**: It uses the QANDA pairs generated by `run_qanda.py` as input to a pre-trained LLM. The LLM is prompted to classify the original claim based on the provided QANDA evidence.
    *   **Input**: The `qanda_results.csv` file.
    *   **Output**: A CSV file (`classification_results.csv`) containing the original claims, their original labels, and the final classification assigned by the LLM.

*   **`util/merge.py`**:
    *   **Role**: A utility script to merge multiple Pandas DataFrame pickle (`.pkl`) files from a specified folder into a single output pickle file.
    *   **Details**: This is particularly useful if `filter_docs.py` is run in parallel parts (using `mod` and `mine` arguments), producing several partial output files that need to be consolidated.
    *   **Input**: A folder path containing `.pkl` files and an output file path.
    *   **Output**: A single merged `.pkl` file.

## Running the Scripts

### Dependencies

*   Python 3.x
*   Key Python libraries:
    *   `pandas`
    *   `transformers` (from Hugging Face)
    *   `torch` (PyTorch)
    *   `fasttext`
    *   `numpy`
    *   `scipy`
    *   `nltk` (Natural Language Toolkit)
    *   `Levenshtein`
    *   `sentence-transformers` (Note: imported in `filter_docs.py`, though primary embedding might be FastText)

    It is recommended to create a virtual environment and install dependencies using a `requirements.txt` file (you would need to generate this, e.g., via `pip freeze > requirements.txt` after installing packages).
    Example installation: `pip install pandas transformers torch fasttext numpy scipy nltk Levenshtein sentence-transformers`

### Prerequisites

1.  **NLTK Data**:
    *   The scripts `filter_docs.py` requires NLTK resources `punkt` (for tokenization) and `stopwords`. The script attempts to download these automatically if not found. You can also manually download them:
        ```python
        import nltk
        nltk.download('punkt')
        nltk.download('stopwords')
        ```

2.  **FastText Model**:
    *   `filter_docs.py` requires a FastText word embedding model. It is configured to use `cc.en.64.bin`. If this file is not present, the script will attempt to download the full `cc.en.300.bin` model, reduce its dimensionality to 64, and save it as `cc.en.64.bin`. Ensure you have internet access and sufficient disk space for the initial download (~7GB for the full model).

3.  **Pre-trained LLM**:
    *   The scripts `run_qanda.py`, `run_rephrasals.py`, and `run_finalclassification.py` use a pre-trained model from Hugging Face Hub, specified as `jpangas/autotrain-llama-cpsc`. The `transformers` library will automatically download this model on its first use. Ensure internet connectivity.

4.  **Input Data Files**:
    *   Ensure all necessary input data files (see "Data Files" section below) are present in the correct locations and formats.

### Execution Order & Examples

A typical execution pipeline would be:

1.  **Filter Documents (`filter_docs.py`)**:
    *   This script processes claims and documents to extract relevant evidence.
    *   It can be run in parallel parts. For example, to process 1/3 of the training data:
        ```bash
        python filter_docs.py train 3 0 y  # For part 0 of 3, using rephrasals
        python filter_docs.py train 3 1 y  # For part 1 of 3, using rephrasals
        python filter_docs.py train 3 2 y  # For part 2 of 3, using rephrasals
        ```
    *   The arguments are: `<split> <mod> <mine> <use_rephrasals_y_n>`.
        *   `<split>`: `train`, `val`, or `test`.
        *   `<mod>`: Number of parallel workers/parts.
        *   `<mine>`: Worker ID/part number (0 to mod-1).
        *   `<use_rephrasals_y_n>`: `y` or `n` to use claim rephrasals during filtering.

2.  **Merge Filtered Results (Optional - `util/merge.py`)**:
    *   If `filter_docs.py` was run in parts, use `merge.py` to combine the output `.pkl` files.
        ```bash
        python util/merge.py results/train/ merged_train_filtered.pkl
        ```
    *   Arguments: `<input_folder> <output_file>`.

3.  **Generate QANDA Pairs (`run_qanda.py`)**:
    *   Uses the output from `filter_docs.py` (potentially merged).
        ```bash
        # Assuming merged_val_filtered.pkl is the input for validation claims
        # and it's renamed/placed as val.withextracts.pickle for the script
        python run_qanda.py 
        ```
    *   This script currently has hardcoded input `val.withextracts.pickle`. You may need to adjust the `pickle_file_path` variable within the script or rename your input file.

4.  **Generate Rephrasals (Optional - `run_rephrasals.py`)**:
    *   Generates rephrased versions of claims.
        ```bash
        # Also uses val.withextracts.pickle by default
        python run_rephrasals.py
        ```
    *   Similar to `run_qanda.py`, it uses a hardcoded input file path.

5.  **Final Classification (`run_finalclassification.py`)**:
    *   Uses the QANDA pairs generated by `run_qanda.py`.
        ```bash
        python run_finalclassification.py
        ```
    *   This script expects `qanda_results.csv` to be present (output of `run_qanda.py`).

## Data Files

The project uses and generates several data files:

*   **Input to `filter_docs.py`**:
    *   **Claim Data**: JSON files (e.g., `baseline/AVeriTeC/data/dev.json`, `train.json`, `test.json`) containing lists of claims, each with an ID, text, and other metadata.
    *   **Knowledge Store**: ZIP archives (e.g., `baseline/AVeriTeC/data_store/knowledge_store/dev_knowledge_store.zip`) containing JSON files, where each file corresponds to a claim ID and contains evidence documents (often one JSON object per line).
    *   **Precomputed Similarity (Optional)**:
        *   `similarity_data/sim.words.txt`: List of words.
        *   `similarity_data/sim.values.txt`: Flattened matrix of precomputed similarity scores for words in `sim.words.txt`.
    *   **Claim Rephrasals (Optional)**:
        *   `rephrasals/<split>.with_rephrasals.json`: JSON file containing claim IDs and lists of their rephrasals, used if the `use_rephrasals` flag in `filter_docs.py` is set to 'y'.

*   **Output of `filter_docs.py`**:
    *   Pickle files (e.g., `results/<split>/<split>_claims_processed_mod_X_mine_Y_FINAL.pkl`). These files store Pandas DataFrames containing the original claims, their labels, and a new field `filtered_docs` which holds a list of processed document dictionaries. Each processed document dictionary includes top matching text spans found by the script.

*   **Input to `run_qanda.py`**:
    *   A pickle file (hardcoded as `val.withextracts.pickle`). This file should be a DataFrame where each row contains at least `claim` (text) and `relevant_docs` (a list of relevant document extracts/strings). This is typically derived from the output of `filter_docs.py`.

*   **Output of `run_qanda.py`**:
    *   `qanda_results.csv`: A CSV file where each row contains the original claim information, the specific document extract used as evidence, and the LLM-generated QANDA pair based on that claim-document pair.

*   **Input to `run_rephrasals.py`**:
    *   A pickle file (hardcoded as `val.withextracts.pickle`) containing claims data (at least a "claim" column).

*   **Output of `run_rephrasals.py`**:
    *   `rephrasals_allresults.csv`: A CSV file containing the original claim data along with a column (`rephrasals`) that holds the string of LLM-generated rephrasals for each claim.

*   **Input to `run_finalclassification.py`**:
    *   `qanda_results.csv` (the output of `run_qanda.py`).

*   **Output of `run_finalclassification.py`**:
    *   `classification_results.csv`: A CSV file containing the original claim information, its original label, and the `classification` (e.g., Supported, Refuted) assigned by the LLM based on the QANDA pairs.

Remember to adjust hardcoded file paths within the scripts if your data is located elsewhere or named differently.
