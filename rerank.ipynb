{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf9af1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2430d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a945007b",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = pd.read_pickle(\"rephrasals/val_extracts.rephrased.pickle\")\n",
    "test_data = pd.read_pickle(\"results/test_extracts.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d6caa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091a142e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = val_data.drop_duplicates(subset=[\"claim_id\"], keep=\"last\")\n",
    "test_data = test_data.drop_duplicates(subset=[\"claim_id\"], keep=\"last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431a6bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_max_score(data):\n",
    "    \"\"\"\n",
    "    Sorts a list of dictionaries by the maximum score among 'mid_lev_scores', 'high_lev_scores', and 'low_lev_scores'.\n",
    "\n",
    "    Args:\n",
    "        data (list): List of dictionaries containing 'top_n' with 'mid_lev_scores', 'high_lev_scores', and 'low_lev_scores'.\n",
    "\n",
    "    Returns:\n",
    "        list: Sorted list of dictionaries.\n",
    "    \"\"\"\n",
    "    def get_max_score(item):\n",
    "        top_n = item.get('top_n', {})\n",
    "        scores = []\n",
    "        for level in ['low_lev_scores', 'mid_lev_scores', 'high_lev_scores']:\n",
    "            if level in top_n and top_n[level]:\n",
    "                scores.append(top_n[level][0][0] if top_n[level][0][1] and len(top_n[level][0][1].split()) >= 50 else float('-inf'))  # Extract the score or set to -inf if conditions are not met\n",
    "        return max(scores) if scores else float('-inf')\n",
    "\n",
    "    return sorted(data, key=get_max_score, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b625520",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_highest_score_text(top_n):\n",
    "    merged_text = \"\"\n",
    "    for level in ['low_lev_scores', 'mid_lev_scores', 'high_lev_scores']:\n",
    "        highest_score_text = \"\"\n",
    "        highest_score = float('-inf')\n",
    "        if level in top_n and top_n[level]:\n",
    "            for score, text in top_n[level]:\n",
    "                if score > highest_score:\n",
    "                    highest_score = score\n",
    "                    highest_score_text = text\n",
    "        if highest_score_text:\n",
    "            merged_text += f\"\\n...{highest_score_text}...\"\n",
    "    \n",
    "    merged_text = merged_text.lstrip(\"\\n\")\n",
    "    merged_text = merged_text.rstrip(\"\\n\")\n",
    "        \n",
    "    return merged_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52cb796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(merge_highest_score_text(q[0].get(\"top_n\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e97dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_max_score(data):\n",
    "    \"\"\"\n",
    "    Sorts a list of dictionaries by the maximum score among 'mid_lev_scores', 'high_lev_scores', and 'low_lev_scores'.\n",
    "\n",
    "    Args:\n",
    "        data (list): List of dictionaries containing 'top_n' with 'mid_lev_scores', 'high_lev_scores', and 'low_lev_scores'.\n",
    "\n",
    "    Returns:\n",
    "        list: Sorted list of dictionaries.\n",
    "    \"\"\"\n",
    "    def get_max_score(item):\n",
    "        top_n = item.get('top_n', {})\n",
    "        scores = []\n",
    "        for level in ['low_lev_scores', 'mid_lev_scores', 'high_lev_scores']:\n",
    "            if level in top_n and top_n[level]:\n",
    "                scores.append(top_n[level][0][0] if top_n[level][0][1] and len(top_n[level][0][1].split()) >= 50 else float('-inf'))  # Extract the score or set to -inf if conditions are not met\n",
    "        return max(scores) if scores else float('-inf')\n",
    "\n",
    "    return sorted(data, key=get_max_score, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ae3733",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_relevant_text_list(row):\n",
    "    filtered_docs = row[\"filtered_docs\"]\n",
    "    sorted_docs = sort_by_max_score(filtered_docs)\n",
    "    extracts = []\n",
    "    for doc in sorted_docs[:5]:\n",
    "        this_extract = merge_highest_score_text(doc.get(\"top_n\", {}))\n",
    "        if this_extract:\n",
    "            extracts.append(this_extract)\n",
    "    return extracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48945ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[\"relevant_docs\"] = test_data.apply(lambda row: create_relevant_text_list(row), axis=1)\n",
    "val_data[\"relevant_docs\"] = val_data.apply(lambda row: create_relevant_text_list(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224b8ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data.to_pickle(\"rephrasals/val.rephrased.withextracts.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822382d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.to_pickle(\"results/test.withextracts.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f258b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20ee933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[\"relevant_docs\"] = data.apply(lambda row: create_relevant_text_list(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e215918c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c188bc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "with open(\"/Users/tomi_owolabi/projects/cpsc601/baseline/AVeriTeC/data/dev.json\") as f:\n",
    "    dev_tasks = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ff2776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_doc(doc):\n",
    "    items = list(doc.values())\n",
    "    items.sort(key = lambda x: x.get(\"top_n\", {}).get(\"low_level_scores\", [-10])[0])\n",
    "    first = items[0]\n",
    "    ret_docs = []\n",
    "    for first in items[:3]:\n",
    "        str_val = \"\"\n",
    "        print(first.get(\"top_n\"))\n",
    "        for k, v in first.get(\"top_n\", {}).items():\n",
    "            for doc_list in v:\n",
    "                # print(doc_list)\n",
    "                str_val += f\"\\n{doc_list[1]}\"\n",
    "        ret_docs.append(str_val)\n",
    "    return ret_docs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce10f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb773ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_with_rephrasals = pd.read_csv(\"rephrasals/validation.rephrasals.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17383116",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_with_rephrasals.sort_values(by=[\"claim_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e40382c",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_with_rephrasals.drop_duplicates(subset=[\"claim_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d46137",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_rephrasals(text):\n",
    "    \"\"\"\n",
    "    Extracts numbered items from a formatted string.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input string containing numbered items.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of extracted items.\n",
    "    \"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        return []\n",
    "    # Use regex to match numbered items\n",
    "    pattern = r'\\d+\\.\\s(.*?)\\n'\n",
    "    matches = re.findall(pattern, text + '\\n')  # Add newline to ensure last item is captured\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd7b798",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_with_rephrasals[\"rephrasal_list\"] = doc_with_rephrasals[\"rephrasals\"].apply(extract_rephrasals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c557944",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_with_rephrasals.sort_values(by=['claim_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5130fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data.sort_values(by=[\"claim_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b570b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rephrasals_csv = pd.read_csv(\"rephrasals/val.rephrasals.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dae887",
   "metadata": {},
   "outputs": [],
   "source": [
    "rephrasals_csv.sort_values(by=[\"claim_id\"], inplace=True)\n",
    "rephrasals_csv =rephrasals_csv.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed630b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "rephrasals_csv[\"rephrasal_list\"] =  rephrasals_csv[\"rephrasals\"].apply(extract_rephrasals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fd3489",
   "metadata": {},
   "outputs": [],
   "source": [
    "rephrasals_csv.to_json(\"rephrasals/val.with_rephrasals.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d202a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rephrasal_df = rephrasals_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa716d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(zip(rephrasal_df[\"claim_id\"], rephrasal_df[\"rephrasal_list\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5d7c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_question_answer(text):\n",
    "    \"\"\"\n",
    "    Parses a string containing a question and an answer into a dictionary.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input string containing the question and answer.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with 'question' and 'answer' keys.\n",
    "    \"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        return {\"question\": \"\", \"answer\": \"\"}\n",
    "    pattern = r\"<question>\\s*(.*?)\\s*<answer>\\s*(.*)\"\n",
    "    match = re.search(pattern, text, re.DOTALL|re.MULTILINE)\n",
    "    if match:\n",
    "        return {\n",
    "            \"question\": match.group(1).strip(),\n",
    "            \"answer\": match.group(2).strip()\n",
    "        }\n",
    "    return {\"question\": \"\", \"answer\": \"\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98e8138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_save_json(class_no_rephrasal_path, qa_no_rephrasal_path, output_json_path, fill=500):\n",
    "    \"\"\"\n",
    "    Processes two CSV files, merges them, and saves the aggregated data as a JSON file.\n",
    "\n",
    "    Args:\n",
    "        class_no_rephrasal_path (str): Path to the classification CSV file.\n",
    "        qa_no_rephrasal_path (str): Path to the QA CSV file.\n",
    "        output_json_path (str): Path to save the output JSON file.\n",
    "    \"\"\"\n",
    "    class_no_rephrasal = pd.read_csv(class_no_rephrasal_path)\n",
    "    qa_no_rephrasal = pd.read_csv(qa_no_rephrasal_path)\n",
    "    no_rephrasal_df = class_no_rephrasal.merge(qa_no_rephrasal, on=[\"claim_id\", \"label\", \"claim\"])\n",
    "    no_rephrasal_df[\"parsed_qa\"] = no_rephrasal_df[\"qanda\"].apply(parse_question_answer)\n",
    "    \n",
    "    aggregated_data = no_rephrasal_df.groupby(\"claim_id\").agg({\n",
    "        \"document\": list,\n",
    "        \"qanda\": list,\n",
    "        \"parsed_qa\": list,\n",
    "        \"relevant_docs\": list,\n",
    "        \"filtered_docs\": \"first\",\n",
    "        \"classification\": \"first\",\n",
    "        \"label\": \"first\",\n",
    "        \"claim\": \"first\"\n",
    "    }).reset_index()\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Fill in missing claim_ids within the range of 500\n",
    "    all_claim_ids = set(range(1, fill))\n",
    "    existing_claim_ids = set(aggregated_data[\"claim_id\"])\n",
    "    missing_claim_ids = all_claim_ids - existing_claim_ids\n",
    "    print(missing_claim_ids)\n",
    "\n",
    "    # for missing_id in missing_claim_ids:\n",
    "    #     aggregated_data = pd.concat([\n",
    "    #         aggregated_data,\n",
    "    #         pd.DataFrame([{\n",
    "    #             \"claim_id\": missing_id,\n",
    "    #             \"parsed_qa\": [],\n",
    "    #             \"filtered_docs\": \"\",\n",
    "    #             \"classification\": \"\",\n",
    "    #             \"pred_label\": \"\",\n",
    "    #             \"claim\": \"\"\n",
    "    #         }])\n",
    "    #     ], ignore_index=True)\n",
    "    \n",
    "    output_json = aggregated_data.apply(\n",
    "        lambda row: {\n",
    "            \"claim_id\": row[\"claim_id\"],\n",
    "            \"claim\": row[\"claim\"],\n",
    "            \"pred_label\": row[\"classification\"],\n",
    "            \"evidence\": row[\"parsed_qa\"]\n",
    "        },\n",
    "        axis=1\n",
    "    ).tolist()\n",
    "\n",
    "    # Save the JSON to a file\n",
    "    with open(output_json_path, \"w\") as f:\n",
    "        json.dump(output_json, f, indent=4)\n",
    "    \n",
    "    return aggregated_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee25dc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_rephrasals_args = [\"results/dev/rephrasals/classification.rephrased.csv\", \"results/dev/rephrasals/qanda.rephrased.csv\", \"results/dev/rephrasals/dev.rephrasals.averitec.json\"]\n",
    "no_rephrasals_args = [\"results/dev/\"]\n",
    "data = process_and_save_json(*with_rephrasals_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89dfbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_no_rephrasals_args = [\"results/dev/test/classification_testset_results.csv\", \"results/dev/test/qanda_testset_results.csv\", \"results/dev/test/test.averitec.json\"]\n",
    "data_test = process_and_save_json(*test_set_no_rephrasals_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e66dd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80e8925",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"results/dev/rephrasals/dev.qa.rephrasals.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff377e9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fd8901",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"results/dev/test/qanda_testset_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2fb8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6fd3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# List of claim_ids to filter out\n",
    "claim_ids_to_filter = [161, 35, 261, 135, 361, 42, 235, 461, 142, 335, 242, 435, 499, 342, 442, 61]\n",
    "\n",
    "{161, 35, 261, 135, 361, 42, 235, 461, 142, 335, 242, 435, 499, 342, 442, 61}\n",
    "\n",
    "\n",
    "# Read the original JSON file\n",
    "with open('baseline/AVeriTeC/data/dev.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "filtered_data = []\n",
    "\n",
    "for idx, x in enumerate(data):\n",
    "    if idx in claim_ids_to_filter:\n",
    "        continue\n",
    "    else:\n",
    "        filtered_data.append(x)\n",
    "\n",
    "# Filter out records with the specified claim_ids\n",
    "# filtered_data = [record for record in data if record['claim_id'] not in claim_ids_to_filter]\n",
    "\n",
    "# Write the filtered data to a new JSON file\n",
    "with open('baseline/AVeriTeC/data/dev.filtered.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(filtered_data, f, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae690510",
   "metadata": {},
   "outputs": [],
   "source": [
    "[{'question': 'What are the policies of Joe Biden regarding immigration?', 'answer': 'Joe Biden\\'s immigration policies include:\\n\\n* Repealing the travel ban on countries in Africa and the Middle East\\n* Ending the Trump administration\\'s \"zero-tolerance\" policy at the southern border\\n* Reinstating the Deferred Action for Childhood Arrivals (DACA) program\\n* Providing a pathway to citizenship for undocumented immigrants\\n* Increasing the number of refugees admitted to the United States\\n* Implementing a merit-based immigration system\\n* Repealing the Trump administration\\'s public charge rule\\n* Repealing the Trump administration\\'s travel ban on countries in Africa and the Middle East\\n* Increasing funding for border security and immigration enforcement\\n* Creating a pathway to citizenship for undocumented immigrants\\n* Implementing a pathway to citizenship for undocumented immigrants\\n* Repealing the Trump administration\\'s travel ban on countries in Africa and the Middle East\\n* Implementing a pathway to citizenship for undocumented immigrants'}, {'question': 'Does Joe Biden support open borders?', 'answer': 'Joe Biden supports a pathway to citizenship for undocumented immigrants.'}, {'question': 'Does Joe Biden support open borders?', 'answer': '...the border is a national security issue, and we will take the necessary steps to secure the border...'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285d9d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpsc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
